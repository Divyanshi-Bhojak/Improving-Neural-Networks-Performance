# Performance improvement of Neural Networks

For better performance comparison analysis used two different datasets CIFAR-10 and SVHN on three different optimization algorithms for Cyclical learning rate as well as Fixed learning rate. We implemented three optimizers SGD with momentum,Adam amd AdamW  for cyclical learning rate and fixed learning rate. Cyclical learning rate with Adam with weight decay performed better and gave highest accuracy when compared with others optimization algorithms.
